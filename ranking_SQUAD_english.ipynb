{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import ezodf\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "import time\n",
    "import ast\n",
    "import json\n",
    "\n",
    "import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "\n",
    "from elasticsearch_dsl import MultiSearch, Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean indices from elastic_search object\n",
    "def delete_all_indices(elastic_search):\n",
    "    for index in list(elastic_search.indices.get_alias('*').keys()):\n",
    "        elastic_search.indices.delete(index)\n",
    "        elastic_search.indices.clear_cache()\n",
    "    return elastic_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SQUAD_TRAIN = 'train-v2.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:14: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "\n",
    "with open(PATH_TO_SQUAD_TRAIN) as f:\n",
    "    SQuAD = json.load(f)['data']\n",
    "    \n",
    "squad_df = json_normalize(SQuAD,\n",
    "    record_path=['paragraphs', 'qas', 'answers'], \n",
    "    meta=[\n",
    "        'title', \n",
    "        ['paragraphs', 'context'], \n",
    "        ['paragraphs', 'qas', 'id'], \n",
    "        ['paragraphs', 'qas', 'is_impossible'], \n",
    "        #['paragraphs', 'qas', 'plausible_answers'], \n",
    "        ['paragraphs', 'qas', 'question']\n",
    "    ],\n",
    "    #errors='ignore'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_squad_df = squad_df[squad_df.title == squad_df.title.loc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "squad_df_pairs = _squad_df[['paragraphs.qas.question', 'paragraphs.context']]\n",
    "squad_df_pairs['question'] = squad_df_pairs['paragraphs.qas.question']\n",
    "squad_df_docs = _squad_df['paragraphs.context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "squad_df_pairs['doc_id'] = squad_df_pairs['paragraphs.context'].apply(lambda x: squad_df_docs[squad_df_docs == x].index.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import analyzer, token_filter\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_index_settings(settings, special_properties, simple_properties, spec={'type': 'text', \n",
    "                                                                                 'analyzer': 'my_analyzer'}):\n",
    "    _props = defaultdict(dict)\n",
    "    for prop in special_properties:\n",
    "        _props[prop] = spec\n",
    "    for prop in simple_properties:\n",
    "        _props[prop] = { 'type': 'text'}\n",
    "        \n",
    "    return {\n",
    "        'settings': settings,\n",
    "        'mappings': {\n",
    "            '_doc': {\n",
    "                'properties': dict(_props)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "def create_query_body(query_type, field_name, query):\n",
    "    ### main case with fuzzy\n",
    "    if query_type == 'fuzzy':\n",
    "        return {\n",
    "            'query': {\n",
    "                'match': {\n",
    "                    field_name: {\n",
    "                       'query': query,\n",
    "                       'fuzziness': 2,\n",
    "                       'prefix_length': 3,\n",
    "                       'max_expansions': 300,\n",
    "                       'operator': 'or',\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    else:\n",
    "        return {}\n",
    "    \n",
    "    \n",
    "def search_param(query_type, field_name, query, index):\n",
    "    return {\n",
    "        'query_body': create_query_body(query_type, field_name, query),\n",
    "        'index_name': index,\n",
    "        'search_field': field_name,\n",
    "    }\n",
    "\n",
    "def stack_uneven(arrays, fill_value=0.):\n",
    "    '''\n",
    "    Fits arrays into a single numpy array, even if they are\n",
    "    different sizes. `fill_value` is the default value.\n",
    "\n",
    "    Args:\n",
    "            arrays: list of np arrays of various sizes\n",
    "                (must be same rank, but not necessarily same size)\n",
    "            fill_value (float, optional):\n",
    "\n",
    "    Returns:\n",
    "            np.ndarray\n",
    "    '''\n",
    "    sizes = [a.shape for a in arrays]\n",
    "    max_sizes = np.max(list(zip(*sizes)), -1)\n",
    "    # The resultant array has stacked on the first dimension\n",
    "    result = np.full((len(arrays),) + tuple(max_sizes), fill_value)\n",
    "    for i, a in enumerate(arrays):\n",
    "      # The shape of this array `a`, turned into slices\n",
    "      slices = tuple(slice(0,s) for s in sizes[i])\n",
    "      # Overwrite a block slice of `result` with this array `a`\n",
    "      result[i][slices] = a\n",
    "    return result\n",
    "\n",
    "def correct_answer_ids(current_page_numbers, current_document_name, dataset):\n",
    "    page_number_mask = dataset['page_numbers_list'].apply(lambda x: len(set(current_page_numbers).intersection(x)) > 0)\n",
    "    document_name_mask = dataset['document_name'].apply(lambda x: x == current_document_name)\n",
    "    exact_df = dataset[page_number_mask & document_name_mask]\n",
    "    return set(exact_df.index)\n",
    "\n",
    "def full_metrics_count(relevance_matrix):\n",
    "    results = []\n",
    "    raw_relevance_matrix = relevance_matrix.copy()\n",
    "    print ('NDCG-5: ', np.array(list(map(lambda x: ndcg_at_k(x, 5), raw_relevance_matrix))).mean())\n",
    "    results.append(np.array(list(map(lambda x: ndcg_at_k(x, 5), raw_relevance_matrix))).mean())\n",
    "    print ('NDCG-10: ', np.array(list(map(lambda x: ndcg_at_k(x, 10), raw_relevance_matrix))).mean())\n",
    "    results.append(np.array(list(map(lambda x: ndcg_at_k(x, 10), raw_relevance_matrix))).mean())\n",
    "    print ('NDCG-20: ', np.array(list(map(lambda x: ndcg_at_k(x, 20), raw_relevance_matrix))).mean())\n",
    "    results.append(np.array(list(map(lambda x: ndcg_at_k(x, 20), raw_relevance_matrix))).mean())\n",
    "    return results\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return np.sum(np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    idcg = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not idcg:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / idcg\n",
    "\n",
    "def print_metric_results(full_scores_df, columns, N_list):\n",
    "    for column in columns:\n",
    "        current_positions = full_scores_df.sort_values(['question', column], ascending=False).\\\n",
    "            groupby('question')['is_correc_answer'].apply(np.array).apply(np.argmax).values\n",
    "        print (column)\n",
    "        for N in N_list:\n",
    "            print ('TOP-{}: {}'.format(N, (current_positions < N).mean() * 100))\n",
    "        \n",
    "        \n",
    "        current_df = full_scores_df.sort_values(['question', column], ascending=False)\n",
    "        relevances_matrix = current_df.groupby('question')['is_correc_answer'].apply(np.array).values\n",
    "        relevances_matrix = np.stack(relevances_matrix, axis=0) #if not use_stack_uneven else stack_uneven(relevances_matrix)\n",
    "        full_metrics_count(relevances_matrix)\n",
    "        print('\\n\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([\"http://admin:admin@localhost:9200\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = squad_df_docs\n",
    "ANALYSIS_list = [\n",
    "    {\n",
    "            \"filter\": {\n",
    "                \"en_US\": {\n",
    "                    \"type\": \"hunspell\",\n",
    "                    \"language\": \"en-US\"\n",
    "                },\n",
    "                \"ru\": {\n",
    "                    \"type\": \"hunspell\",\n",
    "                    \"language\": \"ru\"\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"en_US\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [ \"lowercase\", \"en_US\" ]\n",
    "                },\n",
    "                \"ru\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [ \"lowercase\", \"ru\" ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    \n",
    "    {\n",
    "            \"analyzer\" : {\n",
    "                \"eng_analyzer\" : {\n",
    "                    \"tokenizer\" : \"standard\",\n",
    "                    \"filter\" : [\"lowercase\", \"eng_stemmer\"]\n",
    "                },\n",
    "                \"ru_analyzer\" : {\n",
    "                    \"tokenizer\" : \"standard\",\n",
    "                    \"filter\" : [\"lowercase\", \"ru_stemmer\"]\n",
    "                }\n",
    "            },\n",
    "            \"filter\" : {\n",
    "                \"eng_stemmer\" : {\n",
    "                    \"type\" : \"stemmer\",\n",
    "                    \"name\" : \"english\"\n",
    "                },\n",
    "                \"ru_stemmer\" : {\n",
    "                    \"type\" : \"stemmer\",\n",
    "                    \"name\" : \"russian\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields=['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DELETE http://localhost:9200/searchguard [status:403 request:0.010s]\n"
     ]
    },
    {
     "ename": "AuthorizationException",
     "evalue": "AuthorizationException(403, 'security_exception', 'no permissions for [] and User [name=admin, backend_roles=[admin], requestedTenant=null]')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthorizationException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-d493d7199bf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Delete indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdelete_all_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-3176d56e5c91>\u001b[0m in \u001b[0;36mdelete_all_indices\u001b[0;34m(elastic_search)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdelete_all_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melastic_search\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melastic_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_alias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0melastic_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0melastic_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0melastic_search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/elasticsearch/client/utils.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/elasticsearch/client/indices.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(self, index, params)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty value passed for a required argument 'index'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         return self.transport.perform_request(\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0;34m\"DELETE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_make_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         )\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/elasticsearch/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                     \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m                 )\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/elasticsearch/connection/http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             )\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         self.log_request_success(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/elasticsearch/connection/base.py\u001b[0m in \u001b[0;36m_raise_error\u001b[0;34m(self, status_code, raw_data)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         raise HTTP_EXCEPTIONS.get(status_code, TransportError)(\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         )\n",
      "\u001b[0;31mAuthorizationException\u001b[0m: AuthorizationException(403, 'security_exception', 'no permissions for [] and User [name=admin, backend_roles=[admin], requestedTenant=null]')"
     ]
    }
   ],
   "source": [
    "# Delete indices\n",
    "delete_all_indices(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83aedf3f0a654ac288b279f4f9acdb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dae035985354eba9aa857989c6343fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=753), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b83f5f4399a4e1cab381a283d89daba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=753), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_indexes = defaultdict(str)\n",
    "    \n",
    "for e, i in enumerate(ANALYSIS_list):\n",
    "    _settings = {\n",
    "        'index':{\n",
    "            'blocks':{\n",
    "                'read_only_allow_delete' : 'false'\n",
    "            }\n",
    "        },\n",
    "        'number_of_replicas':0,\n",
    "        'number_of_shards': 1,\n",
    "        \"analysis\": i\n",
    "    }\n",
    "\n",
    "\n",
    "    _ = str(e) + '_index'\n",
    "    \n",
    "    _props = {'type': 'text', \n",
    "                  'analyzer': list(i['analyzer'].keys())[0], \n",
    "                 }\n",
    "    new_indexes[_.lower()] = create_index_settings(_settings, \n",
    "                                             fields,\n",
    "                                             ['document_name'],\n",
    "                                             _props)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "global _temp\n",
    "_temp = new_indexes\n",
    "\n",
    "# Create new indices\n",
    "for index_name, index_settings in tqdm_notebook(new_indexes.items()):\n",
    "    es.indices.create(index_name, body=index_settings, include_type_name=True)\n",
    "    for i in tqdm_notebook(dataset.index):\n",
    "        es.index(index=index_name, doc_type='_doc',\n",
    "                 body={'context': dataset[i]}, id=i)\n",
    "    es.indices.refresh(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameters for queries\n",
    "search_params_dict = defaultdict(str)\n",
    "for i in new_indexes.keys():\n",
    "    for f in fields:\n",
    "        search_params_dict[i + '_' + f] = search_param('fuzzy', f, 'question', i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74dc0a5f5adc41d3bd3151f435c4d98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=752), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create names and dataframe for scores\n",
    "score_columns = [col + '_score' for col in list(search_params_dict.keys())]\n",
    "global full_scores_df\n",
    "full_scores_df = pd.DataFrame(columns=(['question', \n",
    "                                        'doc_id'] + score_columns))\n",
    "\n",
    "\n",
    "questions_list = []\n",
    "document_ids_list = []\n",
    "document_names_list = []\n",
    "document_pages_list = []\n",
    "\n",
    "for question in tqdm_notebook(squad_df_pairs['question'].value_counts().index):\n",
    "    for i in dataset.index:\n",
    "        questions_list.append(question)\n",
    "        document_ids_list.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5c6e58f3614371ad094f80dad0d7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# fill dataframe values\n",
    "full_scores_df['question'] = questions_list\n",
    "full_scores_df['doc_id'] = document_ids_list\n",
    "for score_column in tqdm_notebook(score_columns):\n",
    "    full_scores_df[score_column] = 0\n",
    "\n",
    "### create index    \n",
    "full_scores_df['index'] = list(map(lambda x1, x2: \n",
    "                                   (x1, x2), full_scores_df['question'], full_scores_df['doc_id']))\n",
    "full_scores_df.set_index('index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "752"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(questions_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_questions = set(questions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b6db519c4b41f7a0078724f6b8d20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=752), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Eighth part\n",
    "### iterating by question\n",
    "for i in tqdm_notebook(set_questions):\n",
    "    current_question = i\n",
    "    current_indexes = list(map(lambda x: (current_question, x), dataset.index))\n",
    "\n",
    "\n",
    "    ### iterating by search parameters\n",
    "    for search_name, search_params in search_params_dict.items():\n",
    "        ### create query body\n",
    "        current_query_body = search_params['query_body']\n",
    "        current_query_body['query']['match'][search_params['search_field']]['query'] = current_question\n",
    "\n",
    "        ### run search\n",
    "        \n",
    "\n",
    "        search_results = es.search(index=search_params['index_name'], body=current_query_body, \n",
    "                                   params={'size': len(dataset), 'search_type' : 'dfs_query_then_fetch'})\n",
    "        search_results = search_results['hits']['hits']\n",
    "        ### save results to dataset\n",
    "        ### fill values in dataframe\n",
    "        scores_list = list(map(lambda x: x['_score'], search_results))\n",
    "        ids_list = list(map(lambda x: int(x['_id']), search_results))\n",
    "        score_dict = dict(zip(ids_list, scores_list))\n",
    "        full_scores_df.loc[current_indexes, search_name + '_score'] = list(map(lambda x: \n",
    "                                                                               score_dict.get(x, 0), \n",
    "                                                                               dataset.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'IB', 'distribution': 'll', 'lambda': 'df', 'normalization': 'h1'}] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nineth part\n",
    "\n",
    "### now, we need create labels for (q,d) pairs - some of them would be 1, for correct documents.\n",
    "\n",
    "### we just checked numbers of pages - if intersetion of question pages and document pages is not null, \n",
    "### then this pair is correct\n",
    "squad_df_pairs['correct_answer_id'] = squad_df_pairs['doc_id']\n",
    "\n",
    "\n",
    "correct_answers_dict = squad_df_pairs.set_index('question')['correct_answer_id'].to_dict()\n",
    "full_scores_df['is_correc_answer'] = full_scores_df.apply(lambda x: \n",
    "                                                          1 if x.doc_id in \n",
    "                                                          [correct_answers_dict[x.question]] else 0, axis=1)\n",
    "\n",
    "print(SIMS, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_index_context_score\n",
      "TOP-1: 64.49468085106383\n",
      "TOP-5: 64.49468085106383\n",
      "TOP-10: 66.22340425531915\n",
      "TOP-20: 76.99468085106383\n",
      "NDCG-5:  0.6449468085106383\n",
      "NDCG-10:  0.6502566136762865\n",
      "NDCG-20:  0.6785123577495887\n",
      "\n",
      "\n",
      "\n",
      "1_index_context_score\n",
      "TOP-1: 65.55851063829788\n",
      "TOP-5: 65.55851063829788\n",
      "TOP-10: 66.88829787234043\n",
      "TOP-20: 76.72872340425532\n",
      "NDCG-5:  0.6555851063829787\n",
      "NDCG-10:  0.6596364080508115\n",
      "NDCG-20:  0.6853464288507691\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_metric_results(full_scores_df, score_columns, [1, 5, 10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
