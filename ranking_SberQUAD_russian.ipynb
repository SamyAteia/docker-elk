{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import ezodf\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "import time\n",
    "import ast\n",
    "import json\n",
    "\n",
    "import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "\n",
    "from elasticsearch_dsl import MultiSearch, Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean indices from elastic_search object\n",
    "def delete_all_indices(elastic_search):\n",
    "    for index in list(elastic_search.indices.get_alias('*').keys()):\n",
    "        elastic_search.indices.delete(index)\n",
    "        elastic_search.indices.clear_cache()\n",
    "    return elastic_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SQUAD_TRAIN = '../train-v1.1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:13: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "\n",
    "with open(PATH_TO_SQUAD_TRAIN) as f:\n",
    "    SberQuAD = json.load(f)['data']\n",
    "    \n",
    "sberquad_df = json_normalize(SberQuAD,\n",
    "    record_path=['paragraphs', 'qas', 'answers'], \n",
    "    meta=[\n",
    "        'title', \n",
    "        ['paragraphs', 'context'], \n",
    "        ['paragraphs', 'qas', 'id'], \n",
    "        #['paragraphs', 'qas', 'plausible_answers'], \n",
    "        ['paragraphs', 'qas', 'question']\n",
    "    ],\n",
    "    #errors='ignore'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import analyzer, token_filter\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def create_index_settings(settings, special_properties, simple_properties, spec={'type': 'text', \n",
    "                                                                                 'analyzer': 'my_analyzer'}):\n",
    "    _props = defaultdict(dict)\n",
    "    for prop in special_properties:\n",
    "        _props[prop] = spec\n",
    "    for prop in simple_properties:\n",
    "        _props[prop] = { 'type': 'text'}\n",
    "        \n",
    "    return {\n",
    "        'settings': settings,\n",
    "        'mappings': {\n",
    "            '_doc': {\n",
    "                'properties': dict(_props)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def create_query_body(query_type, field_name, query):\n",
    "    ### main case with fuzzy\n",
    "    if query_type == 'fuzzy':\n",
    "        return {\n",
    "            'query': {\n",
    "                'match': {\n",
    "                    field_name: {\n",
    "                       'query': query,\n",
    "                       'fuzziness': 2,\n",
    "                       'prefix_length': 3,\n",
    "                       'max_expansions': 300,\n",
    "                       'operator': 'or',\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    else:\n",
    "        return {}\n",
    "    \n",
    "    \n",
    "def search_param(query_type, field_name, query, index):\n",
    "    return {\n",
    "        'query_body': create_query_body(query_type, field_name, query),\n",
    "        'index_name': index,\n",
    "        'search_field': field_name,\n",
    "    }\n",
    "\n",
    "\n",
    "def stack_uneven(arrays, fill_value=0.):\n",
    "    '''\n",
    "    Fits arrays into a single numpy array, even if they are\n",
    "    different sizes. `fill_value` is the default value.\n",
    "\n",
    "    Args:\n",
    "            arrays: list of np arrays of various sizes\n",
    "                (must be same rank, but not necessarily same size)\n",
    "            fill_value (float, optional):\n",
    "\n",
    "    Returns:\n",
    "            np.ndarray\n",
    "    '''\n",
    "    sizes = [a.shape for a in arrays]\n",
    "    max_sizes = np.max(list(zip(*sizes)), -1)\n",
    "    # The resultant array has stacked on the first dimension\n",
    "    result = np.full((len(arrays),) + tuple(max_sizes), fill_value)\n",
    "    for i, a in enumerate(arrays):\n",
    "      # The shape of this array `a`, turned into slices\n",
    "      slices = tuple(slice(0,s) for s in sizes[i])\n",
    "      # Overwrite a block slice of `result` with this array `a`\n",
    "      result[i][slices] = a\n",
    "    return result\n",
    "\n",
    "\n",
    "def correct_answer_ids(current_page_numbers, current_document_name, dataset):\n",
    "    page_number_mask = dataset['page_numbers_list'].apply(lambda x: len(set(current_page_numbers).intersection(x)) > 0)\n",
    "    document_name_mask = dataset['document_name'].apply(lambda x: x == current_document_name)\n",
    "    exact_df = dataset[page_number_mask & document_name_mask]\n",
    "    return set(exact_df.index)\n",
    "\n",
    "\n",
    "def full_metrics_count(relevance_matrix):\n",
    "    results = []\n",
    "    raw_relevance_matrix = relevance_matrix.copy()\n",
    "    print ('NDCG-5: ', np.array(list(map(lambda x: ndcg_at_k(x, 5), raw_relevance_matrix))).mean())\n",
    "    results.append(np.array(list(map(lambda x: ndcg_at_k(x, 5), raw_relevance_matrix))).mean())\n",
    "    print ('NDCG-10: ', np.array(list(map(lambda x: ndcg_at_k(x, 10), raw_relevance_matrix))).mean())\n",
    "    results.append(np.array(list(map(lambda x: ndcg_at_k(x, 10), raw_relevance_matrix))).mean())\n",
    "    print ('NDCG-20: ', np.array(list(map(lambda x: ndcg_at_k(x, 20), raw_relevance_matrix))).mean())\n",
    "    results.append(np.array(list(map(lambda x: ndcg_at_k(x, 20), raw_relevance_matrix))).mean())\n",
    "    print('\\n\\n')\n",
    "    return results\n",
    "\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return np.sum(np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    idcg = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not idcg:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / idcg\n",
    "\n",
    "\n",
    "def print_metric_results(full_scores_df, columns, N_list):\n",
    "    _metrics = []\n",
    "    for column in columns:\n",
    "        current_positions = full_scores_df.sort_values(['question', column], ascending=False).\\\n",
    "            groupby('question')['is_correc_answer'].apply(np.array).apply(np.argmax).values\n",
    "        print (column)\n",
    "        for N in N_list:\n",
    "            print ('TOP-{}: {}'.format(N, (current_positions < N).mean() * 100))\n",
    "        \n",
    "        \n",
    "        current_df = full_scores_df.sort_values(['question', column], ascending=False)\n",
    "        relevances_matrix = current_df.groupby('question')['is_correc_answer'].apply(np.array).values\n",
    "        relevances_matrix = np.stack(relevances_matrix, axis=0) #if not use_stack_uneven else stack_uneven(relevances_matrix)\n",
    "        _metrics.append(full_metrics_count(relevances_matrix))\n",
    "    return _metrics\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([\"http://admin:admin@localhost:9200\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYSIS_list = [\n",
    "    {\n",
    "            \"filter\": {\n",
    "                \"en_US\": {\n",
    "                    \"type\": \"hunspell\",\n",
    "                    \"language\": \"en-US\"\n",
    "                },\n",
    "                \"ru\": {\n",
    "                    \"type\": \"hunspell\",\n",
    "                    \"language\": \"ru\"\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"en_US\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [ \"lowercase\", \"en_US\" ]\n",
    "                },\n",
    "                \"ru\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [ \"lowercase\", \"ru\" ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    \n",
    "    {\n",
    "            \"analyzer\" : {\n",
    "                \"eng_analyzer\" : {\n",
    "                    \"tokenizer\" : \"standard\",\n",
    "                    \"filter\" : [\"lowercase\", \"eng_stemmer\"]\n",
    "                },\n",
    "                \"ru_analyzer\" : {\n",
    "                    \"tokenizer\" : \"standard\",\n",
    "                    \"filter\" : [\"lowercase\", \"ru_stemmer\"]\n",
    "                }\n",
    "            },\n",
    "            \"filter\" : {\n",
    "                \"eng_stemmer\" : {\n",
    "                    \"type\" : \"stemmer\",\n",
    "                    \"name\" : \"english\"\n",
    "                },\n",
    "                \"ru_stemmer\" : {\n",
    "                    \"type\" : \"stemmer\",\n",
    "                    \"name\" : \"russian\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields=['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sberquad_df.shape[0]//500+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_groups = sberquad_df.shape[0]//500+1\n",
    "\n",
    "ndcg_list = []\n",
    "\n",
    "for _i in range(number_of_groups):\n",
    "\n",
    "    _squad_df = sberquad_df.loc[_i*500:(_i+1)*500]\n",
    "\n",
    "    squad_df_pairs = _squad_df[['paragraphs.qas.question', 'paragraphs.context']]\n",
    "    squad_df_pairs['question'] = squad_df_pairs['paragraphs.qas.question']\n",
    "    squad_df_docs = _squad_df['paragraphs.context']\n",
    "\n",
    "    squad_df_pairs['doc_id'] = squad_df_pairs['paragraphs.context'].apply(lambda x: \n",
    "                                                        squad_df_docs[squad_df_docs == x].index.values[0])\n",
    "    dataset = squad_df_docs\n",
    "    try:\n",
    "        # Delete indices\n",
    "        delete_all_indices(es)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    new_indexes = defaultdict(str)\n",
    "\n",
    "    for e, i in enumerate(ANALYSIS_list):\n",
    "        _settings = {\n",
    "            'index':{\n",
    "                'blocks':{\n",
    "                    'read_only_allow_delete' : 'false'\n",
    "                }\n",
    "            },\n",
    "            'number_of_replicas':0,\n",
    "            'number_of_shards': 1,\n",
    "            \"analysis\": i\n",
    "        }\n",
    "\n",
    "\n",
    "        _ = str(e) + '_index'\n",
    "\n",
    "        _props = {'type': 'text', \n",
    "                      'analyzer': list(i['analyzer'].keys())[1], \n",
    "                     }\n",
    "        new_indexes[_.lower()] = create_index_settings(_settings, \n",
    "                                                 fields,\n",
    "                                                 ['document_name'],\n",
    "                                                 _props)\n",
    "\n",
    "    # Create new indices\n",
    "    for index_name, index_settings in tqdm_notebook(new_indexes.items()):\n",
    "        es.indices.create(index_name, body=index_settings, include_type_name=True)\n",
    "        for i in tqdm_notebook(dataset.index):\n",
    "            es.index(index=index_name, doc_type='_doc',\n",
    "                     body={'context': dataset[i]}, id=i)\n",
    "        es.indices.refresh(index_name)\n",
    "        \n",
    "\n",
    "    # Create parameters for queries\n",
    "    search_params_dict = defaultdict(str)\n",
    "    for i in new_indexes.keys():\n",
    "        for f in fields:\n",
    "            search_params_dict[i + '_' + f] = search_param('fuzzy', f, 'question', i)\n",
    "\n",
    "    # Create names and dataframe for scores\n",
    "    score_columns = [col + '_score' for col in list(search_params_dict.keys())]\n",
    "    global full_scores_df\n",
    "    full_scores_df = pd.DataFrame(columns=(['question', \n",
    "                                            'doc_id'] + score_columns))\n",
    "\n",
    "\n",
    "    questions_list = []\n",
    "    document_ids_list = []\n",
    "    document_names_list = []\n",
    "    document_pages_list = []\n",
    "\n",
    "    for question in tqdm_notebook(squad_df_pairs['question'].value_counts().index):\n",
    "        for i in dataset.index:\n",
    "            questions_list.append(question)\n",
    "            document_ids_list.append(i)\n",
    "\n",
    "    # fill dataframe values\n",
    "    full_scores_df['question'] = questions_list\n",
    "    full_scores_df['doc_id'] = document_ids_list\n",
    "    for score_column in tqdm_notebook(score_columns):\n",
    "        full_scores_df[score_column] = 0\n",
    "\n",
    "    ### create index    \n",
    "    full_scores_df['index'] = list(map(lambda x1, x2: \n",
    "                                       (x1, x2), full_scores_df['question'], full_scores_df['doc_id']))\n",
    "    full_scores_df.set_index('index', inplace=True)\n",
    "\n",
    "    set_questions = set(questions_list)\n",
    "\n",
    "    # Eighth part\n",
    "    ### iterating by question\n",
    "    for i in tqdm_notebook(set_questions):\n",
    "        current_question = i\n",
    "        current_indexes = list(map(lambda x: (current_question, x), dataset.index))\n",
    "\n",
    "\n",
    "        ### iterating by search parameters\n",
    "        for search_name, search_params in search_params_dict.items():\n",
    "            ### create query body\n",
    "            current_query_body = search_params['query_body']\n",
    "            current_query_body['query']['match'][search_params['search_field']]['query'] = current_question\n",
    "\n",
    "            ### run search\n",
    "\n",
    "\n",
    "            search_results = es.search(index=search_params['index_name'], body=current_query_body, \n",
    "                                       params={'size': len(dataset), 'search_type' : 'dfs_query_then_fetch'})\n",
    "            search_results = search_results['hits']['hits']\n",
    "            ### save results to dataset\n",
    "            ### fill values in dataframe\n",
    "            scores_list = list(map(lambda x: x['_score'], search_results))\n",
    "            ids_list = list(map(lambda x: int(x['_id']), search_results))\n",
    "            score_dict = dict(zip(ids_list, scores_list))\n",
    "            full_scores_df.loc[current_indexes, search_name + '_score'] = list(map(lambda x: \n",
    "                                                                                   score_dict.get(x, 0), \n",
    "                                                                                   dataset.index))\n",
    "\n",
    "    # Nineth part\n",
    "\n",
    "    ### now, we need create labels for (q,d) pairs - some of them would be 1, for correct documents.\n",
    "\n",
    "    ### we just checked numbers of pages - if intersetion of question pages and document pages is not null, \n",
    "    ### then this pair is correct\n",
    "    squad_df_pairs['correct_answer_id'] = squad_df_pairs['doc_id']\n",
    "\n",
    "\n",
    "    correct_answers_dict = squad_df_pairs.set_index('question')['correct_answer_id'].to_dict()\n",
    "    full_scores_df['is_correc_answer'] = full_scores_df.apply(lambda x: \n",
    "                                                              1 if x.doc_id in \n",
    "                                                              [correct_answers_dict[x.question]] else 0, axis=1) \n",
    "    \n",
    "    print(f'{_i*500}-{(_i+1)*500}\\n\\n')\n",
    "    ndcg_list.append(print_metric_results(full_scores_df, score_columns, [1, 5, 10, 20]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 2, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(ndcg_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97546959, 0.97674733, 0.97738175],\n",
       "       [0.97548501, 0.97684416, 0.97746666]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ndcg_list, axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
